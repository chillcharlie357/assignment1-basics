# Tokenizer configuration
# Used by both dataset loading and tokenizer instantiation
vocab_path: "data/vocab/TinyStoriesV2-GPT4-train_vocab.pkl"
merges_path: "data/vocab/TinyStoriesV2-GPT4-train_merges.pkl"
special_tokens: ["<|endoftext|>"]

# Training data (for tokenizer training)
input_path: "data/TinyStoriesV2-GPT4-train.txt"
vocab_size: 10000
