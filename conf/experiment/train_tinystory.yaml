# @package _global_

# uv run cs336_basics/scripts/train_llm.py experiment=train_tinystory
training:
  epochs: 1
  batch_size: 320
  learning_rate: 1e-3
tokenizer:
  vocab_path: "data/vocab/TinyStoriesV2-GPT4-train_vocab.pkl"
  merges_path: "data/vocab/TinyStoriesV2-GPT4-train_merges.pkl"
  special_tokens: ["<|endoftext|>"]
  vocab_size: 10000
model:
  num_layers: 4
  num_heads: 16
  d_model: 512
  d_ff: 1344
  theta: 10000
  max_seq_len: 256

logging:
  name: "train_tinystory"
  level: "INFO"
